# 自动编码器
   如果给定一个神经网络，我们假设其输入与输出是相同的，然后训练调整其参数，得到每一层的权重。自然的，
我们就得到输入I的几种不同的表示，这些表示就是特征。自动编码器就是一种尽可能复现输入信号的神经网络。
为了实现这种复现，自动编码器就必须捕捉可以表示输入数据的最重要的因素，就像PCA一样，可以找到代表原始
信息的主要成分。[^1]
```flow 
   input=>start:Input
   error=>end:Error
   encoder=>subroutine:EnCoder
   decoder=>subroutine:Decoder
   input->encoder->decoder->error 
   input->error
```
   我们将input输入一个编码器就会得到一个编码，再加入一个解码器就会重新解码，重构出这份数据。因为是
无标签数据，如果输出的数据和原始输入相比误差很小，我们就得到了不错的编码器。我们可以通过编码器产生
特征，然后训练下一层。这样进行逐层的训练。  
   这样逐层的训练之后，我们就可以得到很多层了。这时还不能直接用于分类，因为它还没有学习如何去连结一
个输入和一个类。它只是学会了如何去重构和复现它的输入。或者说，它只是学习获取了一个可以良好代表输入
的特征，这个特征可以最大限度的表示原始输入信号。  
   如果在这个基础上给'编码器部分'加上L1的规范化约束，使大部分的节点参数都是0，只有少数不是0，这样就
得到稀疏自动编码器。还有一种叫`Denoising AutoEncoders`就是在训练数据中加入噪声，自动编码器必须学习去
除这种噪声而获得真正的没有被噪声污染过的输入。这就迫使编码器去学习输入信号更健壮的表达，所以泛化能力
也比一般的编码器强。DA可以通过梯度下降法进行训练。




[^1]: http://www.cnblogs.com/rong86/p/3555290.html
