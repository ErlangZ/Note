# 卷积运算Convolution $(f*g)(x)$ 

在泛函分析中，卷积是通过两个函数$f$和$g$生成第三个函数的数学运算，表征函数f和经过翻转，平移的g的乘积函数
围成的曲边梯形的面积。  
连续函数卷积: 设$f(x)$,$g(x)$是$R$上两个*可积函数*，作积分：$$\int_{-\infty}^{\infty}f(t)g(x-t)dt$$
离散序列卷积: 设$f(m)$,$g(n)$是长度为$N$的两个离散信号，它们的积分是$$(f*g)[n] = \sum_{m=0}^{N-1}f(m)g(n-m)$$
卷积运算的应用相当广泛，比如多项式的乘法（包括整数的乘法）,其实就是在进行对两个离散序列进行卷积运算。在图像
处理中，用作图像的边缘检测，图像模糊，锐化等等。在统计学中，加权的平滑是一种卷积。在概率论中，两个统计独立
的变量X，Y的概率密度函数是XY概率密度函数的卷积。在信号处理的过程中，任何一个系统的输出都可以看成是输入信号
和系统冲击响应的卷积。
## 离散卷积信号的推导[^1]
我们从0时刻开始1s向一片原本平静的水面丢一块`单位质量`石头，我们把水面的反应看成一种固定大小的*冲击响应*。水面在t=0时刻
丢进去的时候会激起高度为$h(0)$的波纹，但水面不会立刻归于平静，随着时间的流逝，波纹会越来越小。$h(t)$是t时刻,我们最开始时
刻丢弃的`单位质量`石子引起的波浪高度,$h(t)$被成为冲击响应。  
我们假设$x(t)$是t时刻丢弃的石子质量，$y(t)$是t时刻的波浪高度。$$y(0)=x(0)h(0)$$ $$y(1)=x(0)h(1)+x(1)h(0)$$ ...
$$y(t)=x(0)h(t-1)+x(1)h(t-2)+...+x(t-1)h(0)$$ 那么,$$y(t) =(x*h)[t]$$

## 离散卷积的计算
1. 直接计算
2. 快速傅里叶变换[^2]
3. 分段卷积（具体不太了解，对f分段同g卷积，再把结果相加）

## 卷积的性质
各种卷积都满足如下的性质  
- 交换率 $$f*g = g*f$$
- 结合率 $$f*(g*h) = (f*g)*h$$
- 分配率 $$f*(g+h) = (f*g) + f*h$$
- 数乘结合率 $$a(f*g)=(af)*g=f*(ag)$$
- 微分定理 $$D(f*g)=Df*g=f*Dg$$ 其中, $Df$表示f的微分，如果离散域中,则表示对f的差分。  
  前向差分:$D^+f(n)=f(n+1)-f(n)$  
  后向差分:$D^-f(n)=f(n)-f(n-1)$  

## 二维离散卷积的计算
1. 朴素方法  
以图像处理过程为例，A(m,n)代表一幅M X N的单通道图片, B(s,t)表示$S \times T$的卷积核。
$$C(m,n)=\sum_{i=0}^{S-1}\sum_{j=0}^{T-1}A(m-i,n-j)B(i,j)$$ 同一维的卷积类似,二维的卷积先将卷积核进行对角的翻转
（横向翻转180度，再纵向旋转180度），再进行滑动的乘法运算。更高维的卷积也是遵循类似的规则进行计算。  
假设有一幅$H\times W \times D$的图片，单个卷积核的大小为$K \times K$, 在图片上使用M个卷积核,Stride=1时，计算方法如下:
```
for w in 1...W
    for h in 1...H
        for x in 1...K
        for y in 1...K
            for m in 1...M
            for d in 1...D
                output(w,h,m) += input(w+x,h+y,d) * filter(m, x, y, d)
            end
            end
        end
        end
    end
end
``` 
根据上边的公式，我们也可以计算出卷积结果大小,在任意维度上都是 (axis_length - kernel_size) / stride + 1 。

2. 化归矩阵乘法[^4]
在卷积神经网络框架Caffe中是采用了这种方法计算卷积,实际上，最开始Caffe使用的是朴素的计算方法，在使用过程中发现了
很多同内存相关的问题。于是，Caffe的作者采用了Matlab中采用了一个trick：Matlab im2col操作, 卷积变成了如下的两步运算：
- 调用im2col操作将$N \times H \times W$的图像转换为  
  im2col操作对图像块的行列重新进行排列[^5]。B=im2col(A,[m,n],block_type)对图像块进行了重新的排列。block_type是描述
  重排列方法的特征值。默认的方式其实就是按照每行依次首位相接的方式。$$A = \begin{matrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{matrix}$$
  $$im2col(A)=[A_{11} A_{12} A_{21} A_{22}]$$ 具体到Caffe中, 预处理分了两步:  
  拉平卷积核： M个$k \times k$卷积核，变换成$M \times K$的矩阵。其中，$K=k\times k$。如果M=1,其实就是将卷积核拉平。  
  拉平图像：针对$H \times W$的图像, 变换成$K \times N$的矩阵。其中$N=((H+2*Pad_h-k_h)/stride_h+1)\times((W+2*Pad_w-k_w)/stride_w+1)$  
  N实际上是卷积核在输入图像上滑动截取的特征数，K行表示每个$k \times k$的卷积核在图像上对应的数据。
- 应用BLAS中的GEMM函数计算两个矩阵乘法  

举个例子  
两个卷积核$\begin{matrix}1&0\\2&1\end{matrix}$和$\begin{matrix}2&3\\1&1\end{matrix}$，就是说,$M=2$, $k_h=2$, $k_w=2$, K=4
图像矩阵的值为$\begin{matrix}3&1&2\\1&0&1\\2&1&3\end{matrix}$，H=3,W=3。我们简单点，不用填充边缘，让stride=1。
卷积核转换的A矩阵$M \times K$是 $$A=\begin{matrix}1&0&2&1\\2&3&1&1\end{matrix}$$
图像转换成的B矩阵$K \times N$是 $$B=\begin{matrix}3&1&1&0\\1&2&0&1\\1&0&2&1\\0&1&1&3\end{matrix}$$
然后直接计算$A \times B$得到的矩阵$\begin{matrix}5&2&6&5\\10&9&5&7\end{matrix}$包含了卷积值$\begin{matrix}5&2\\6&5\end{matrix}$和
$\begin{matrix}10&9\\5&7\end{matrix}$ 


## 卷积定理
时间域卷积等价于频率域相乘: $$f(t)*g(t)=F(w)G(w)$$

## 反卷积 Deconvolution[^3]
在信号处理的时候，经常会遇到卷积反演的问题，即根据已知系统的输入和输出信号求解系统的冲击响应函数。 形式化表示为
$$g(n)=(h*f)[n]$$ 已知g(n)和f(n)，求解h(n)。 通常对于卷积的反演是利用离散傅立叶变换(DFT)进行的。经过DFT之后，
上式表示为 $$G(k)=H(k)F(k)$$ $$H(K)=G(k)/F(k)$$再对$H(k)$ 进行IDFT就可以得到$h(n)$。
**参考资料中提到的这个方法的潜在问题我们不考虑。在卷积神经网络中，De-Convolution并不是真正指反卷积，而是另外
一种运算称为Transposed-Convolution。**

[^1]: 知乎-定义卷积时为什么要对其中一个函数进行翻转(question:20500497)-from:中微子 
[^2]: https://erlangz.wordpress.com/FFT.html
[^3]: 《一种频域卷积反演的新方法》--from:中国科学院电子所-王岩飞-1996
[^4]: https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo
[^5]: http://cn.mathworks.com/help/images/ref/im2col.html





